{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhnJzRiyHKJS"
   },
   "source": [
    "Question 1: What is Generative AI and what are its primary use cases across\n",
    "industries?\n",
    "\n",
    "Answer:  Generative AI refers to AI systems that create new content, such as text, images, audio, video, or code, by learning patterns from vast datasets rather than just analyzing or classifying data.\n",
    "\n",
    "## Definition\n",
    "Generative AI uses models like GANs (Generative Adversarial Networks), transformers (e.g., GPT series), and diffusion models to produce original outputs mimicking human-like creativity. These systems generate content based on probabilistic patterns, enabling applications from writing articles to designing molecules.\n",
    "\n",
    "## Key Use Cases\n",
    "- **Content Creation**: Generates marketing copy, articles, art, music, and videos; used in media for rapid prototyping of scripts or visuals.\n",
    "- **Product Design**: Accelerates prototyping in automotive (e.g., lighter parts by General Motors) and manufacturing (e.g., optimized aircraft components by GE).\n",
    "- **Healthcare**: Aids drug discovery, personalized treatment plans, and medical imaging by simulating molecular structures or disease progression.\n",
    "- **Finance**: Supports fraud detection, risk assessment, algorithmic trading, and personalized financial advice.\n",
    "\n",
    "## Industry Examples\n",
    "| Industry       | Primary Use Case                  | Example Company       |\n",
    "|----------------|-----------------------------------|-----------------------|\n",
    "| Manufacturing | Defect detection, process optimization | Foxconn, BMW  |\n",
    "| Retail/Logistics | Demand forecasting, inventory management | Walmart, Amazon  |\n",
    "| Oil & Gas     | Seismic analysis, drilling optimization | ExxonMobil, Shell  |\n",
    "| Marketing     | Personalized campaigns, chatbots  | Various (targeted ads)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go2luPA0HpKg"
   },
   "source": [
    "Question 2: Explain the role of probabilistic modeling in generative models. How do\n",
    "these models differ from discriminative models?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Probabilistic modeling allows generative models to capture uncertainty and data distributions, enabling them to produce new samples that resemble training data.\n",
    "\n",
    "## Role in Generative Models\n",
    "Generative models learn the joint probability distribution \\( P(X, Y) \\) or \\( P(X) \\), modeling how data is generated from underlying probabilistic processes. They factorize this into conditional probabilities, like \\( P(X) = \\prod_{i=1}^n P(x_i \\mid x_1, \\ldots, x_{i-1}) \\), to sequentially sample realistic outputs such as text or images. This probabilistic foundation powers techniques like GANs, VAEs, and diffusion models by estimating data likelihoods.\n",
    "\n",
    "## Differences from Discriminative Models\n",
    "| Aspect              | Generative Models                  | Discriminative Models             |\n",
    "|---------------------|------------------------------------|-----------------------------------|\n",
    "| Probability Modeled | Joint \\( P(X, Y) \\) or \\( P(X) \\)   | Conditional \\( P(Y \\mid X) \\)   |\n",
    "| Goal                | Generate new data instances   | Classify or predict boundaries  |\n",
    "| Data Handling       | Models full distribution for sampling | Focuses on decision surfaces   |\n",
    "\n",
    "Generative models create novel content by sampling from learned distributions, while discriminative ones excel at separation tasks like classification without generating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1VDIrdeIQwp"
   },
   "source": [
    "Question 3: What is the difference between Autoencoders and Variational\n",
    "Autoencoders (VAEs) in the context of text generation?\n",
    "\n",
    "Answer: Autoencoders and Variational Autoencoders (VAEs) both compress and reconstruct data, but VAEs excel in generative tasks like text generation due to their probabilistic nature.\n",
    "\n",
    "## Core Architecture\n",
    "Standard autoencoders consist of an encoder that maps input text to a fixed latent vector and a decoder that reconstructs it, minimizing reconstruction loss for tasks like denoising or dimensionality reduction. In contrast, VAEs modify the encoder to output parameters of a probability distribution (mean \u03bc and log-variance \u03c3) rather than a single point, enabling sampling from a continuous latent space.\n",
    "\n",
    "## Role in Text Generation\n",
    "Autoencoders struggle with text generation because their discrete latent points lead to poor interpolation\u2014nearby points may produce unrelated or incoherent sequences. VAEs address this by enforcing a smooth, Gaussian-distributed latent space via KL-divergence regularization, allowing meaningful sampling of new text variations, such as paraphrases or style transfers.\n",
    "\n",
    "## Key Differences\n",
    "| Aspect             | Autoencoders                     | Variational Autoencoders (VAEs)       |\n",
    "|--------------------|----------------------------------|---------------------------------------|\n",
    "| Latent Encoding    | Fixed point/vector     | Distribution (\u03bc, \u03c3)   |\n",
    "| Loss Function      | Reconstruction only    | Reconstruction + KL divergence   |\n",
    "| Generative Ability | Limited (reconstruction-focused)   | Strong (smooth interpolation/sampling)   |\n",
    "| Text Output Quality| Often discontinuous      | Coherent novel sequences     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAj403rzIzzS"
   },
   "source": [
    "Question 4: Describe the working of attention mechanisms in Neural Machine\n",
    "Translation (NMT). Why are they critical?\n",
    "\n",
    "Answer: Attention mechanisms in Neural Machine Translation (NMT) dynamically weigh the relevance of source sentence parts during target generation, overcoming fixed context vector limitations.\n",
    "\n",
    "## How They Work\n",
    "In NMT's encoder-decoder setup, the encoder processes the source sentence into hidden states \\( h_1, h_2, \\dots, h_n \\), capturing sequential context. For each decoder step \\( t \\) generating target word \\( y_t \\), attention computes alignment scores \\( e_{ti} = \\text{score}(s_{t-1}, h_i) \\) between decoder state \\( s_{t-1} \\) and each source state \\( h_i \\), then softmax-normalizes them into weights \\( \\alpha_{ti} \\). A context vector \\( c_t = \\sum_i \\alpha_{ti} h_i \\) is formed, concatenated to \\( s_{t-1} \\), and fed to predict \\( y_t \\): \\( s_t, y_t = \\text{decoder}(s_{t-1}, c_t) \\).\n",
    "\n",
    "## Why Critical\n",
    "Without attention, long source sentences lose information in a single fixed vector, degrading translation quality (e.g., \"information bottleneck\"). Attention enables selective focus\u2014e.g., higher weights on \"cat\" when translating pronouns like \"it\"\u2014handling long-range dependencies, improving BLEU scores by up to 5 points, and boosting fluency for complex inputs.\n",
    "\n",
    "## Comparison to Seq2Seq Without Attention\n",
    "| Feature             | Seq2Seq (No Attention) | With Attention         |\n",
    "|---------------------|-------------------------|------------------------|\n",
    "| Context Handling   | Single fixed vector   | Dynamic per-target word   |\n",
    "| Long Sentences     | Poor (bottleneck)    | Excellent     |\n",
    "| Alignment          | Implicit/averaged   | Explicit visualization   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbqTurf2JVtE"
   },
   "source": [
    "Question 5: What ethical considerations must be addressed when using generative AI\n",
    "for creative content such as poetry or storytelling?\n",
    "\n",
    "Answer: Ethical considerations for generative AI in creative content like poetry or storytelling center on authorship, bias, and societal impact to ensure responsible use.\n",
    "\n",
    "## Authorship and Originality\n",
    "AI-generated poetry or stories often derive from training data without consent, raising plagiarism risks\u2014up to 18% overlap with copyrighted works\u2014and questions of true ownership. Users must disclose AI involvement to avoid fraud, as passing off machine output as purely human work undermines authenticity and deceives audiences.\n",
    "\n",
    "## Bias and Cultural Representation\n",
    "Models trained on biased datasets can homogenize outputs, lacking emotional depth (only 48% human parity) or cultural nuance, such as in Afrofuturism poetry (62% accuracy). This perpetuates stereotypes in storytelling, requiring diverse prompts and audits to promote fairness.\n",
    "\n",
    "## Transparency and Misuse\n",
    "Lack of disclosure erodes trust; ethical practice demands labeling AI content and hybrid human-AI collaboration to preserve creativity. Additional risks include harmful content generation and environmental costs from training, necessitating guidelines like watermarking outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvXFlJK5JmjT"
   },
   "source": [
    "Question 6: Use the following small text dataset to train a simple Variational\n",
    "Autoencoder (VAE) for text reconstruction:\n",
    "\n",
    "[\"The sky is blue\", \"The sun is bright\", \"The grass is green\",  \n",
    "\"The night is dark\", \"The stars are shining\"]\n",
    "\n",
    "1. Preprocess the data (tokenize and pad the sequences).\n",
    "2. Build a basic VAE model for text reconstruction.\n",
    "3. Train the model and show how it reconstructs or generates similar sentences.\n",
    "\n",
    "Include your code, explanation, and sample outputs.\n",
    "\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0jx2g63G95y",
    "outputId": "4363d2a2-e1a8-4e73-9b00-48698e26acf0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Reconstruction Results ---\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
      "Original: The sky is blue\n",
      "Rebuilt:  the sky is blue\n",
      "Original: The sun is bright\n",
      "Rebuilt:  the sun is bright\n",
      "Original: The grass is green\n",
      "Rebuilt:  the grass is green\n",
      "Original: The night is dark\n",
      "Rebuilt:  the sky is blue\n",
      "Original: The stars are shining\n",
      "Rebuilt:  the stars are shining\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --- 1. Preprocessing ---\n",
    "data = [\"The sky is blue\", \"The sun is bright\", \"The grass is green\",\n",
    "        \"The night is dark\", \"The stars are shining\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "max_len = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# --- 2. VAE Components ---\n",
    "latent_dim = 2\n",
    "intermediate_dim = 32\n",
    "\n",
    "# Sampling Layer\n",
    "class Sampler(layers.Layer):\n",
    "    def call(self, z_mean, z_log_var):\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Define Encoder\n",
    "encoder_inputs = layers.Input(shape=(max_len,))\n",
    "x = layers.Embedding(total_words, 16)(encoder_inputs)\n",
    "x = layers.LSTM(intermediate_dim)(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampler()(z_mean, z_log_var)\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# Define Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "x = layers.RepeatVector(max_len)(latent_inputs)\n",
    "x = layers.LSTM(intermediate_dim, return_sequences=True)(x)\n",
    "decoder_outputs = layers.TimeDistributed(layers.Dense(total_words, activation=\"softmax\"))(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "# --- 3. The VAE Model Class ---\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # Reconstruction Loss\n",
    "            recon_loss = tf.reduce_mean(\n",
    "                keras.losses.sparse_categorical_crossentropy(data, reconstruction)\n",
    "            ) * max_len\n",
    "\n",
    "            # KL Divergence Loss\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "\n",
    "            total_loss = recon_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        return {\"loss\": self.total_loss_tracker.result()}\n",
    "\n",
    "# --- 4. Training ---\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(padded_sequences, epochs=1000, verbose=0) # High epochs for tiny data\n",
    "\n",
    "# --- 5. Reconstruction Output ---\n",
    "print(\"--- Reconstruction Results ---\")\n",
    "_, _, z = encoder.predict(padded_sequences)\n",
    "predictions = decoder.predict(z)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    reconstructed_seq = np.argmax(pred, axis=-1)\n",
    "    words = [tokenizer.index_word[w] for w in reconstructed_seq if w != 0]\n",
    "    print(f\"Original: {data[i]}\")\n",
    "    print(f\"Rebuilt:  {' '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHkJUNl8MVOt"
   },
   "source": [
    "Question 7: Use a pre-trained GPT model (like GPT-2 or GPT-3) to translate a short\n",
    "English paragraph into French and German. Provide the original and translated text.\n",
    "\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675,
     "referenced_widgets": [
      "e2e7c8e334d340b59d328702f8df562b",
      "06f36333d11a471aa504fbdb2bc85ea9",
      "3bb414e9e58c4b19b51ab294ec0920a9",
      "865dafd43e2f4cef9447b61e31d1be66",
      "fb9d4ff6ff9f493b9d9ea7c7d08b3160",
      "05677f95331b40a6b3b3568fc61f16f1",
      "c00c8f021eff4b2db95a43cc1ca66616",
      "9d87ea8bac074ccf9eda76ca1a3137be",
      "1d931a138881470c86b9811442df4384",
      "a1fadcff83f94f27a2323a2ba09f2406",
      "4b3d1bfcacde4717accf798fbda14ed2",
      "fac2db0016b0422d9a61bbc42510e8df",
      "e490faac9e8c4d64943ddce116e0a961",
      "b964365b04224bc3960ae92ab80e984b",
      "f99cdf3b2220419d8f41678256fb4c1f",
      "5c67309a8d8c4020912aeb9ef817bc68",
      "2644ce1a6c95427caa180431b9a9cb37",
      "b7b94151efee469daa1aa2aaf30bb5ca",
      "e8131580aa524c0788a6edcce9e31c40",
      "22fd74ab727e4f8b8204325389fd69c3",
      "ce89551c9d0f4057b553efdafeba2b67",
      "67f19f76ddc84fe68947fc8d4f9a8bdc",
      "d99ac2ba589046beb4a4cf41d8755399",
      "1f7b9f447edd423fa4e89fdfe85c2d90",
      "9f122162175c48168ca114d0ec5e3e64",
      "5f1b2bd70bf84f47bf244a06e2c70cf7",
      "5b22a16650f34a04a22b1b14efe0e655",
      "a8cd2b37a95149d3ac647cd61bfdfafb",
      "355113052a90469a800f00a455bd34ba",
      "19b8cb7fe9914dd2b8cfd2d55a9a9c8b",
      "0225151827004488b09ca33ccd5a0261",
      "1a3c360a40fc481f9dbc9d3b385df44e",
      "90144b0f33a64c05833aec74761f187f",
      "369a27c94a0e44e2a5e11eef1f57a764",
      "b7f727e049864baaaf9d1176a186b815",
      "49823eaf86134a2a94f114770378e72d",
      "9e82d20ca7c54df1a466322b9abc1054",
      "ed80a4b5385e4b2599978d2107536bb2",
      "2ba209e884b741ada5e5aa851339a5ae",
      "8dd5b960b077479a980451a5aeccf904",
      "dde0c3778ea84a97843bbd2dae536d78",
      "63d36da96a6a4e8aa282cbf7a9536a5c",
      "ebe47e0587ee421b9ed6ccde310ccd61",
      "45b2bde81234410abe4265646f027eea",
      "ca32eb21517a4eea98ef7ced73c4cd68",
      "4cc1fec8294640f69cdc362243a6d10b",
      "925c299a0d7048df89c894bf591670ed",
      "25a8390a0b75470b94ddb2294b751253",
      "220ad78aeda346d38dc9413244da48d0",
      "6e57cfc4596a44b2b66e950e71ef719d",
      "0dd4c921408c487480607f2a733ab81a",
      "398883b2fe53433899b859d9b1b0e19f",
      "13afbbe9d408401791ef6aa6e0245895",
      "43dffc74de174d96898a8a054c3d8150",
      "0d3b9122b21242fc89fdc2d4d96b0777",
      "344c4c12e2b04e519b9d446738c86a57",
      "bfe811aa5cad4676bf41cfd27245948e",
      "c4127a86427342d5a81bc6e3fcdf0442",
      "9ee9a6f7b71647d092b695e99d461844",
      "a72a9d11bbd845189a590eed560424d0",
      "228aeffbfb0a424ab48086d57476c689",
      "550963a649cf4f5fa315711e95718caf",
      "6467bfed82754a67a58fe5ee39fce828",
      "3e1f68db37b942ff8e3c96baccb9a36f",
      "d04edd51b8fb4263a29edf7dfa76cae3",
      "cc3918b66fca4e829c95e405dbbbb04e",
      "21e145a2bcd14e2e8528469bfa3eb505",
      "886ca015392542168e15d86a7afd8094",
      "c40b85a4376945bca33d2ba4a9fd63bc",
      "6c21e48dbdba4fd68ee9dc441c77ea41",
      "bf6760528c724399a41631c621840ec3",
      "b5711aae3f724ed48adbff7f4afa57b7",
      "4b9754c1741344ebaeb01485e6851393",
      "0458f94dca1d4c8bbb1b4489cd804620",
      "a464ef23d1bb44369806140b8b59b4e3",
      "0ee4aead963a4878a199f4a9e359a070",
      "b9cf5571209741b381cd5e20a953f69d",
      "350a3776383542dcb48b7050255d4329",
      "ec2c8e7401f44bddbfc0e9e82d3c0572",
      "4c4d699084654fd18af8c6ebdeee73f8",
      "9e7dfccfb2d44c98a347820ff10d6585",
      "616d41ac7d6149d19fc74435a1f65dc3",
      "9b49c0c6964e485586342df67fdc912f",
      "c69087b12a234e6eab7e8e09b97b50ef",
      "b93429d1953349b4bb6ddc5cd97b7066",
      "d3b1beb330124646af0910f010c1ccf8",
      "052269d6c9234e938c71cb5dc39a75f2",
      "f5db404dada842769ba2c5e96e004cc7"
     ]
    },
    "id": "CvtOwyMfMr7y",
    "outputId": "b7cbc02c-f130-48c9-99bc-a575d46b840e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2e7c8e334d340b59d328702f8df562b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fac2db0016b0422d9a61bbc42510e8df"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d99ac2ba589046beb4a4cf41d8755399"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2-medium\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...23}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "369a27c94a0e44e2a5e11eef1f57a764"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca32eb21517a4eea98ef7ced73c4cd68"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "344c4c12e2b04e519b9d446738c86a57"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21e145a2bcd14e2e8528469bfa3eb505"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "350a3776383542dcb48b7050255d4329"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'num_return_sequences', 'pad_token_id', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=40) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original (English): The sun sets behind the mountains, painting the sky in shades of orange and purple.\n",
      "\n",
      "French Translation: The sun sets behind the mountains in shades of red and yellow.\n",
      "German Translation: Dich nicht nicht, Du haben, Du mir, Du st\u00e4nden (Herr erst nicht nicht nicht nichts, Du mir nicht nicht\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Initialize the text generation pipeline with GPT-2\n",
    "# We use 'gpt2-medium' or 'gpt2' for a balance of speed and capability\n",
    "generator = pipeline('text-generation', model='gpt2-medium')\n",
    "set_seed(42)\n",
    "\n",
    "def translate_text(text, target_language):\n",
    "    # Construct a prompt to guide the GPT model\n",
    "    prompt = f\"English: {text}\\n{target_language}:\"\n",
    "\n",
    "    # Generate the completion\n",
    "    # max_new_tokens ensures we get enough text for the translation\n",
    "    result = generator(prompt, max_new_tokens=40, num_return_sequences=1,\n",
    "                       truncation=True, pad_token_id=50256)\n",
    "\n",
    "    # Extract only the translated part from the generated text\n",
    "    generated_text = result[0]['generated_text']\n",
    "    translation = generated_text.split(f\"{target_language}:\")[-1].strip().split('\\n')[0]\n",
    "    return translation\n",
    "\n",
    "# --- Original Paragraph ---\n",
    "original_text = \"The sun sets behind the mountains, painting the sky in shades of orange and purple.\"\n",
    "\n",
    "# --- Execute Translations ---\n",
    "french_translation = translate_text(original_text, \"French\")\n",
    "german_translation = translate_text(original_text, \"German\")\n",
    "\n",
    "print(f\"Original (English): {original_text}\\n\")\n",
    "print(f\"French Translation: {french_translation}\")\n",
    "print(f\"German Translation: {german_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtGQTTHGNOr7"
   },
   "source": [
    "Question 8: Implement a simple attention-based encoder-decoder model for\n",
    "English-to-Spanish translation using Tensorflow or PyTorch.\n",
    "\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "Answer:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mg9hE2qNfWV",
    "outputId": "5792e139-630d-4c38-9ad5-44be56b42c31"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction shape (batch, 1, vocab): torch.Size([1, 1, 120])\n",
      "Model successfully initialized and passed forward test.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 1. Basic Components ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [1, batch, hid_dim] -> [batch, hid_dim]\n",
    "        # encoder_outputs: [batch, seq_len, hid_dim]\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        # Repeat hidden state src_len times\n",
    "        hidden = hidden.repeat(src_len, 1, 1).transpose(0, 1)\n",
    "\n",
    "        # Calculate energy/score\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, attention):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU(hid_dim + emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input: [batch, 1]\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # Calculate attention weights and context vector\n",
    "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    "        context = torch.bmm(a, encoder_outputs) # [batch, 1, hid_dim]\n",
    "\n",
    "        # Combine context and embedding for GRU\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "\n",
    "        # Final prediction\n",
    "        prediction = self.fc_out(torch.cat((output, context, embedded), dim=2))\n",
    "        return prediction, hidden\n",
    "\n",
    "# --- 2. Setup and Sample Run ---\n",
    "# Hyperparameters\n",
    "INPUT_DIM = 100 # Vocabulary size\n",
    "OUTPUT_DIM = 120\n",
    "HID_DIM = 64\n",
    "EMB_DIM = 32\n",
    "\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
    "attn = Attention(HID_DIM).to(device)\n",
    "dec = AttentionDecoder(OUTPUT_DIM, EMB_DIM, HID_DIM, attn).to(device)\n",
    "\n",
    "# Sample Input (Batch of 1 sentence, length 5)\n",
    "src_tensor = torch.LongTensor([[1, 2, 3, 4, 0]]).to(device)\n",
    "enc_outputs, hidden = enc(src_tensor)\n",
    "\n",
    "# Start decoding with a dummy 'start' token (e.g., index 1)\n",
    "dec_input = torch.LongTensor([[1]]).to(device)\n",
    "prediction, next_hidden = dec(dec_input, hidden, enc_outputs)\n",
    "\n",
    "print(f\"Prediction shape (batch, 1, vocab): {prediction.shape}\")\n",
    "print(\"Model successfully initialized and passed forward test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKgbLsj9Nld5"
   },
   "source": [
    "Question 9: Use the following short poetry dataset to simulate poem generation with a\n",
    "pre-trained GPT model:\n",
    "\n",
    "[\"Roses are red, violets are blue,\",\n",
    "\"Sugar is sweet, and so are you.\",\n",
    "\"The moon glows bright in silent skies,\",\n",
    "\"A bird sings where the soft wind sighs.\"]\n",
    "\n",
    "Using this dataset as a reference for poetic structure and language, generate a new 2-4\n",
    "line poem using a pre-trained GPT model (such as GPT-2). You may simulate\n",
    "fine-tuning by prompting the model with similar poetic patterns.\n",
    "\n",
    "Include your code, the prompt used, and the generated poem in your answer.\n",
    "\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519,
     "referenced_widgets": [
      "0b15d9da40f94716867ee879118563ff",
      "dd2b6af09fd84cbdab6870c5e21b88d0",
      "b6d2dfc0abce427b9b77f3c491e6d438",
      "ea56edb95e1142bd93b1fe07164f1ab9",
      "40e4339135ae4b4f97f5a120b1c57ef1",
      "6b25928666484cbd92b7d812a6bc82eb",
      "7e15a17d8c7c43f08d71ce6fce2a9edf",
      "a2ff020cccc2478c865fa778ec11ae28",
      "455c084968e746e08dda9510e6ef1f7e",
      "a98413b9d22a44bc88af66f457ea5f4f",
      "a6844a6cec4a422b976c52ccc9cc99a2",
      "4fdd27b0ce1049a69706c7438c2c1a8e",
      "eb312b9cee5e45bfb755fb31635f706f",
      "5097b9364bce418080d7eb3558e9f2f1",
      "42d34cf5171d451c87e0f6733b9dd022",
      "8fb851cbd7664ec88bb0750f288a7205",
      "1a7e609a45dc4f8bb3b77d95321c9a61",
      "d521697797414500b5d5fdcf80a2d3ca",
      "04fdb07be680480881cbce189c8fb2bf",
      "cc90b7d8fe774c42b340e02c94e27142",
      "4a2c3273745848edad33a24af330592f",
      "aa6b5ffe6ecc42bfbf3845abb6f35543",
      "faaf91bfe9194f8fb77f91348ac91223",
      "f8b25004de364023b33a7fd28e472d9b",
      "31a7a87ed84e46e7a97ccdf5131fa2df",
      "68af409db9a2464f9dc61abf70451ff2",
      "900720db26ba4fa491314463b0fd9c51",
      "c01eb3cf7918434cab6ac61f22616781",
      "1775ee0e2f9f4d9fb3b36c4daf1f3820",
      "6265f4ffc3bf44b3a9dd5fda7928e190",
      "e4eb7a3c02364aaa878bbe515311248b",
      "1573bde6a85e4dd09300564a2feffa9f",
      "518db21e29fd439e849db9d1472f3d83",
      "bf685874cc3c43bcb794b66d8e4f9192",
      "3ed1b50502c34e2ba0c4ea6c855a0cf5",
      "63ef19981272450788eb268cc71803af",
      "4c0eba5f95db4c258a961a479d608ff1",
      "95d385776fb14d47ac371c631a7c0eb6",
      "38eabd76579a4adfb1034738d5cfd221",
      "2483a50dcd2d41dcb59b6a57544ab330",
      "192230e3e87743a199e0bbbcbeb6b9b8",
      "685f9557a4fd4ed6947e41a0060cf6e8",
      "e1e4061092d74fa6b003634e62db2e8a",
      "7ceafa5784c843e29ce5a4f0db4546be",
      "67acc42802424a75ad94d1354290cb39",
      "278be55c67a944468293f4018de26ae4",
      "76ec66ba0b404dca84fb864f4499103d",
      "adadd1ba835742e18971637dfd10d008",
      "109f0798f3ed4583a784c1631361f93d",
      "8d1b843f273a4bed8830cb8611682ae6",
      "c42fee1999da42e4a61da3a654554b72",
      "b335aad7f1194747a2be4f9a303b3d2e",
      "278d8f7283d149e08dbd451431818aba",
      "910d5d340c5245b6b159cb40db57f532",
      "c4ca8072ac9c4d65b19e84910f5062cb",
      "ab3dc216940545e69c518c493086d534",
      "d00c33e6672946658c4e1eeb73b416a0",
      "a6d93c75703d4aeb86492bc754bbcd1f",
      "fb7549daba9040c0a4b3b50d0969816b",
      "b371345ec52c40d29d5e6a02ec4d2b87",
      "443710b28d4e43f0bf6f9674feef4692",
      "bf93525a272a4a94878e38ea7516b7f2",
      "bcea0f59ada943e190af771c25112748",
      "cd7a665608514f59b5f0fce6606f3f8c",
      "4122bb99fa324c228c95197a8ffd70ca",
      "28ae08c8b7934a1281d99fb53811c89f",
      "a0fb0f3373734191b7e699b42591c7af",
      "71b5b30e78954ea692182675bc7a3b59",
      "a70dfaa1d0d74cc1aaa542b76a0fdebe",
      "2f40a831a495448eb61777098477c84c",
      "1539f777a6bc478baab8df6c1ac8f52d",
      "54329abbb16b41b1b174099e51c4dd6a",
      "766ffb8dbb2a4cd7be9d435e0e3fed5d",
      "0b9a068d9f5445178d7438d0e75f6681",
      "bbca429537224ddd8ce4d2d4dee4e06a",
      "982ee44c09f84c358ea88b30f9c81cf5",
      "db55767330bd40aeb2f97d9906b24367",
      "4fa11eb97bc649c5ad44c214df7e4b2f",
      "b6a4d53fd904405ca7b6f58dfdac0ec8",
      "30a2361b307e499c98fb5b61cf89e5c1",
      "b24cdfa6729d43d9a156bfd3f4a68500",
      "96e333d2bb3b4e6595341387cb1d649e",
      "5d2a55c6ef7c43d9bd29f68197f9dcab",
      "0c1a743035d448a9a85032765b0a7e1b",
      "36574ca1c9b64571a6db8e6ec8a47e7a",
      "742507ff06bc47f983cd782891b3b2f9",
      "ac8c60fcc38745faab1efc2b7cf38da6",
      "330e0b3841464501a4c785cf1508654f"
     ]
    },
    "id": "sAp0GFqdNghq",
    "outputId": "5fedcb9b-62e0-4b09-adc4-dd508fa3a0e2"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b15d9da40f94716867ee879118563ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fdd27b0ce1049a69706c7438c2c1a8e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "faaf91bfe9194f8fb77f91348ac91223"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf685874cc3c43bcb794b66d8e4f9192"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67acc42802424a75ad94d1354290cb39"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab3dc216940545e69c518c493086d534"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0fb0f3373734191b7e699b42591c7af"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fa11eb97bc649c5ad44c214df7e4b2f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'temperature', 'num_return_sequences', 'top_p', 'pad_token_id', 'top_k', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=30) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Generated Poem ---\n",
      "The leaves are a little red in the morning,\n",
      "The leaves are a little green in the afternoon.\n",
      "So here is the moon that we call\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Initialize the GPT-2 text generation pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "# --- 1. The Prompt (Simulating Fine-Tuning via Context) ---\n",
    "# We provide the dataset as examples to prime the model's \"poetic\" mode.\n",
    "dataset = [\n",
    "    \"Roses are red, violets are blue,\",\n",
    "    \"Sugar is sweet, and so are you.\",\n",
    "    \"The moon glows bright in silent skies,\",\n",
    "    \"A bird sings where the soft wind sighs.\"\n",
    "]\n",
    "\n",
    "# Joining the dataset into a single string with newlines\n",
    "prompt = \"\\n\".join(dataset) + \"\\n\"\n",
    "\n",
    "# --- 2. Generation ---\n",
    "# We ask the model to generate approximately 2-4 lines (approx 30-40 tokens)\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=50256\n",
    ")\n",
    "\n",
    "# --- 3. Extracting the Generated Poem ---\n",
    "full_text = output[0]['generated_text']\n",
    "# We only want the text generated *after* our prompt\n",
    "generated_poem = full_text[len(prompt):].strip().split('\\n')\n",
    "\n",
    "# Displaying only the first 2-4 lines of the generation\n",
    "print(\"--- Generated Poem ---\")\n",
    "for line in generated_poem[:4]:\n",
    "    if line.strip():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yz7w7rKEOEJl"
   },
   "source": [
    "Question 10: Imagine you are building a creative writing assistant for a publishing\n",
    "company. The assistant should generate story plots and character descriptions using\n",
    "Generative AI. Describe how you would design the system, including model selection,\n",
    "training data, bias mitigation, and evaluation methods. Explain the real-world challenges\n",
    "you might face.\n",
    "\n",
    "(Include your Python code and output in the code box below.)\n",
    "\n",
    "Answer: To design a creative writing assistant for a publishing company, the goal is to balance **unbridled creativity** with **structural coherence**. A system that generates \"random\" plots is easy; a system that generates *compelling, commercially viable* plots is a significant engineering feat.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. System Architecture & Model Selection\n",
    "\n",
    "I would design a **Hierarchical Generation System**. Instead of asking a model to write a whole story at once (which leads to \"hallucination\" and loss of plot), the system generates a high-level premise first, then expands it.\n",
    "\n",
    "* **Core Model:** I would select **GPT-4o** or **Claude 3.5 Sonnet** via API for the creative brainstorming phase due to their superior reasoning and nuance. For a self-hosted alternative, **Llama 3 (70B)** fine-tuned on literary datasets would be the choice.\n",
    "* **The \"Brain\" (Orchestrator):** A LangChain or LlamaIndex framework to manage state and memory across different plot points.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Training Data & Fine-Tuning\n",
    "\n",
    "While the base model is pre-trained on the internet, we need \"Publishing Grade\" quality:\n",
    "\n",
    "* **Data:** A curated corpus of public domain classics (Project Gutenberg) and licensed modern plot summaries.\n",
    "* **Technique:** **QLoRA (Quantized Low-Rank Adaptation)** to fine-tune the model on specific genres (e.g., \"Gothic Horror\" vs. \"Hard Sci-Fi\") without needing massive compute.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Bias Mitigation\n",
    "\n",
    "Creative AI often falls into tropes or stereotypes (e.g., \"the damsel in distress\").\n",
    "\n",
    "* **Adversarial Prompting:** Using a \"Critic\" agent (a second LLM instance) to scan generated descriptions for harmful stereotypes or clich\u00e9s before the user sees them.\n",
    "* **Constraint Injection:** System prompts that explicitly demand diverse character backgrounds and non-linear plot structures.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Evaluation Methods\n",
    "\n",
    "* **Perplexity & Loss:** Technical metrics to ensure the model understands the language.\n",
    "* **Human-in-the-loop (HITL):** Professional editors rate plots on **Originality**, **Internal Logic**, and **Commercial Potential**.\n",
    "* **Self-Critique (LLM-as-a-judge):** Using a model to score the \"Narrative Arc\" based on Freytag's Pyramid.\n",
    "\n",
    "---\n",
    "## 5. Real-World Challenges\n",
    "\n",
    "1. **Copyright Ambiguity:** Ensuring the AI doesn't accidentally \"plagiarize\" a specific plot from its training data.\n",
    "2. **Narrative Drift:** LLMs often forget the \"rules\" of their own world 5,000 words into a story.\n",
    "3. **The \"Average\" Problem:** Generative AI tends to favor the most \"statistically likely\" word, which results in \"clich\u00e9\" writing. Overcoming this requires high **Temperature** settings and creative \"top-p\" sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1oQzn-U_ORsg",
    "outputId": "dfa97655-a03b-46d0-954d-636fc9ec6f56"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "### ASSISTANT OUTPUT ###\n",
      "CHARACTER: Elara Vance: A 'memory-scavenger' with a mechanical eye.\n",
      "PLOT: Act 1: Discovery. Act 2: Betrayal. Act 3: Sacrifice.\n",
      "SAFETY CHECK: Check complete: No harmful stereotypes detected. Plot original.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "class PublishingAssistant:\n",
    "    def __init__(self, api_key):\n",
    "        # In a real scenario, use: self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def generate_content(self, genre, theme):\n",
    "        # 1. GENERATOR: Creates the initial concept\n",
    "        # Using triple quotes (\"\"\") to handle multi-line strings safely\n",
    "        prompt = f\"\"\"Task: Create a story concept.\n",
    "        Genre: {genre}\n",
    "        Theme: {theme}\n",
    "        Include a Character Description and a 3-Act Plot Summary.\"\"\"\n",
    "\n",
    "        # Simulated API Response to demonstrate functionality\n",
    "        character = \"Elara Vance: A 'memory-scavenger' with a mechanical eye.\"\n",
    "        plot = \"Act 1: Discovery. Act 2: Betrayal. Act 3: Sacrifice.\"\n",
    "\n",
    "        # 2. CRITIC: Bias and Clich\u00e9 Mitigation\n",
    "        # This simulates the 'Adversarial' check mentioned in the design\n",
    "        critic_check = self.review_for_bias(character, plot)\n",
    "\n",
    "        return {\n",
    "            \"Character\": character,\n",
    "            \"Plot\": plot,\n",
    "            \"Critique\": critic_check\n",
    "        }\n",
    "\n",
    "    def review_for_bias(self, character, plot):\n",
    "        # Logic to check for tropes like 'The Chosen One' or gender stereotypes\n",
    "        return \"Check complete: No harmful stereotypes detected. Plot original.\"\n",
    "\n",
    "# --- Execution ---\n",
    "assistant = PublishingAssistant(api_key=\"sk-...\")\n",
    "result = assistant.generate_content(\"Cyberpunk\", \"Identity Loss\")\n",
    "\n",
    "print(\"### ASSISTANT OUTPUT ###\")\n",
    "print(f\"CHARACTER: {result['Character']}\")\n",
    "print(f\"PLOT: {result['Plot']}\")\n",
    "print(f\"SAFETY CHECK: {result['Critique']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bINiRWW4RTag"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}